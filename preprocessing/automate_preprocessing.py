# -*- coding: utf-8 -*-
"""automate_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j9S6EwxqJ9JAN8oqivRTeKDATny7E0cu
"""

#!/usr/bin/env python3
"""
Fruit and Vegetable Image Preprocessing Automation

This module provides automated preprocessing functionality for fruit and vegetable
image recognition dataset. It handles data loading, preprocessing, augmentation,
and saving preprocessed data ready for machine learning model training.

Author: Data Science Team
Date: 2025
License: MIT
"""

import os
import json
import pickle
import warnings
from typing import Dict, List, Tuple, Optional, Union

import numpy as np
import cv2
import kagglehub
from sklearn.preprocessing import LabelEncoder

# Suppress warnings
warnings.filterwarnings('ignore')


class FruitVegetablePreprocessor:
    """
    Automated preprocessing class for fruit and vegetable image recognition dataset.

    This class handles the complete preprocessing pipeline including:
    - Dataset downloading from Kaggle
    - Image preprocessing (resize, normalize)
    - Data augmentation for training set
    - Label encoding
    - Saving preprocessed data
    """

    def __init__(self, target_size: Tuple[int, int] = (224, 224),
                 dataset_id: str = "kritikseth/fruit-and-vegetable-image-recognition"):
        """
        Initialize the preprocessor.

        Args:
            target_size: Target image size as (height, width)
            dataset_id: Kaggle dataset identifier
        """
        self.target_size = target_size
        self.dataset_id = dataset_id
        self.price_calorie_map = self._create_price_calorie_mapping()
        self.label_encoder = None
        self.categories = []
        self.dataset_path = None
        self.splits = {}

    def _create_price_calorie_mapping(self) -> Dict[str, Dict[str, Union[int, str]]]:
        """
        Create price and calorie mapping for fruits and vegetables.

        Returns:
            Dictionary containing price (IDR/kg), calories (per 100g), and type info
        """
        return {
            # FRUITS
            'apple': {'price': 25000, 'calories': 52, 'type': 'fruit'},
            'banana': {'price': 15000, 'calories': 89, 'type': 'fruit'},
            'grapes': {'price': 45000, 'calories': 62, 'type': 'fruit'},
            'kiwi': {'price': 35000, 'calories': 61, 'type': 'fruit'},
            'lemon': {'price': 20000, 'calories': 29, 'type': 'fruit'},
            'mango': {'price': 18000, 'calories': 60, 'type': 'fruit'},
            'orange': {'price': 22000, 'calories': 47, 'type': 'fruit'},
            'pear': {'price': 28000, 'calories': 57, 'type': 'fruit'},
            'pineapple': {'price': 12000, 'calories': 50, 'type': 'fruit'},
            'pomegranate': {'price': 40000, 'calories': 83, 'type': 'fruit'},
            'watermelon': {'price': 8000, 'calories': 30, 'type': 'fruit'},

            # VEGETABLES
            'beetroot': {'price': 15000, 'calories': 43, 'type': 'vegetable'},
            'bell pepper': {'price': 25000, 'calories': 31, 'type': 'vegetable'},
            'cabbage': {'price': 8000, 'calories': 25, 'type': 'vegetable'},
            'capsicum': {'price': 22000, 'calories': 31, 'type': 'vegetable'},
            'carrot': {'price': 12000, 'calories': 41, 'type': 'vegetable'},
            'cauliflower': {'price': 14000, 'calories': 25, 'type': 'vegetable'},
            'chilli pepper': {'price': 30000, 'calories': 40, 'type': 'vegetable'},
            'corn': {'price': 10000, 'calories': 86, 'type': 'vegetable'},
            'cucumber': {'price': 8000, 'calories': 16, 'type': 'vegetable'},
            'eggplant': {'price': 12000, 'calories': 25, 'type': 'vegetable'},
            'garlic': {'price': 45000, 'calories': 149, 'type': 'vegetable'},
            'ginger': {'price': 35000, 'calories': 80, 'type': 'vegetable'},
            'jalepeno': {'price': 35000, 'calories': 29, 'type': 'vegetable'},
            'lettuce': {'price': 18000, 'calories': 15, 'type': 'vegetable'},
            'onion': {'price': 15000, 'calories': 40, 'type': 'vegetable'},
            'paprika': {'price': 40000, 'calories': 282, 'type': 'vegetable'},
            'peas': {'price': 20000, 'calories': 81, 'type': 'vegetable'},
            'potato': {'price': 12000, 'calories': 77, 'type': 'vegetable'},
            'raddish': {'price': 10000, 'calories': 16, 'type': 'vegetable'},
            'soy beans': {'price': 25000, 'calories': 147, 'type': 'vegetable'},
            'spinach': {'price': 15000, 'calories': 23, 'type': 'vegetable'},
            'sweetcorn': {'price': 12000, 'calories': 86, 'type': 'vegetable'},
            'sweetpotato': {'price': 15000, 'calories': 86, 'type': 'vegetable'},
            'tomato': {'price': 18000, 'calories': 18, 'type': 'vegetable'},
            'turnip': {'price': 12000, 'calories': 28, 'type': 'vegetable'},
        }

    def download_dataset(self) -> Optional[str]:
        """
        Download dataset from Kaggle.

        Returns:
            Path to downloaded dataset or None if failed
        """
        try:
            print(f"Downloading dataset: {self.dataset_id}")
            path = kagglehub.dataset_download(self.dataset_id)
            print(f"Dataset downloaded successfully to: {path}")
            return path
        except Exception as e:
            print(f"Error downloading dataset: {e}")
            return None

    def load_dataset_structure(self, dataset_path: str) -> bool:
        """
        Load and analyze dataset structure.

        Args:
            dataset_path: Path to the dataset

        Returns:
            True if successful, False otherwise
        """
        if not dataset_path or not os.path.exists(dataset_path):
            print("Invalid dataset path!")
            return False

        self.dataset_path = dataset_path

        # Find train, test, validation folders
        self.splits = {}
        for split_name in ['train', 'test', 'validation']:
            split_path = os.path.join(dataset_path, split_name)
            if os.path.exists(split_path):
                self.splits[split_name] = split_path

        # Get categories from train folder
        if 'train' in self.splits:
            self.categories = [f for f in os.listdir(self.splits['train'])
                             if os.path.isdir(os.path.join(self.splits['train'], f))]
            self.categories.sort()

        print(f"Found {len(self.categories)} categories in {len(self.splits)} splits")
        return True

    def _preprocess_single_image(self, image_path: str) -> Optional[np.ndarray]:
        """
        Preprocess a single image.

        Args:
            image_path: Path to the image file

        Returns:
            Preprocessed image array or None if failed
        """
        try:
            # Load image
            image = cv2.imread(image_path)
            if image is None:
                return None

            # Convert BGR to RGB
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Resize image
            image = cv2.resize(image, self.target_size)

            # Normalize pixel values to [0,1]
            image = image.astype(np.float32) / 255.0

            return image
        except Exception as e:
            print(f"Error processing image {image_path}: {e}")
            return None

    def _apply_augmentation(self, image: np.ndarray) -> List[np.ndarray]:
        """
        Apply data augmentation to an image.

        Args:
            image: Input image array

        Returns:
            List of augmented images including original
        """
        augmented_images = []

        # Original image
        augmented_images.append(image.copy())

        # Horizontal flip
        flipped = cv2.flip(image, 1)
        augmented_images.append(flipped)

        # Rotation (10 degrees)
        rows, cols = image.shape[:2]
        rotation_matrix = cv2.getRotationMatrix2D((cols/2, rows/2), 10, 1)
        rotated = cv2.warpAffine(image, rotation_matrix, (cols, rows))
        augmented_images.append(rotated)

        # Brightness adjustment
        bright = cv2.convertScaleAbs(image, alpha=1.2, beta=0.1)
        bright = np.clip(bright, 0, 1)
        augmented_images.append(bright)

        return augmented_images

    def preprocess_dataset(self, apply_augmentation: bool = True) -> Dict[str, Dict[str, np.ndarray]]:
        """
        Preprocess the entire dataset.

        Args:
            apply_augmentation: Whether to apply augmentation to training data

        Returns:
            Dictionary containing preprocessed data for each split
        """
        if not self.splits or not self.categories:
            raise ValueError("Dataset structure not loaded. Call load_dataset_structure() first.")

        print("Starting dataset preprocessing...")

        processed_data = {
            'train': {'images': [], 'labels': [], 'augmented': apply_augmentation},
            'test': {'images': [], 'labels': [], 'augmented': False},
            'validation': {'images': [], 'labels': [], 'augmented': False}
        }

        # Initialize label encoder
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(self.categories)

        total_processed = 0

        # Process each split
        for split_name, split_path in self.splits.items():
            print(f"Processing {split_name} data...")
            split_processed = 0

            for category in self.categories:
                category_path = os.path.join(split_path, category)

                if not os.path.exists(category_path):
                    continue

                # Get all images in category
                image_files = [f for f in os.listdir(category_path)
                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

                for image_file in image_files:
                    image_path = os.path.join(category_path, image_file)

                    # Preprocess image
                    processed_image = self._preprocess_single_image(image_path)
                    if processed_image is None:
                        continue

                    # Encode label
                    encoded_label = self.label_encoder.transform([category])[0]

                    if split_name == 'train' and apply_augmentation:
                        # Apply augmentation only for training data
                        augmented_images = self._apply_augmentation(processed_image)
                        for aug_image in augmented_images:
                            processed_data['train']['images'].append(aug_image)
                            processed_data['train']['labels'].append(encoded_label)
                            split_processed += 1
                    else:
                        # No augmentation for test and validation
                        processed_data[split_name]['images'].append(processed_image)
                        processed_data[split_name]['labels'].append(encoded_label)
                        split_processed += 1

            total_processed += split_processed
            print(f"  - {split_name}: {split_processed} images processed")

        print(f"Total images processed: {total_processed}")

        # Convert lists to numpy arrays
        for split_name in processed_data.keys():
            if processed_data[split_name]['images']:
                processed_data[split_name]['images'] = np.array(processed_data[split_name]['images'])
                processed_data[split_name]['labels'] = np.array(processed_data[split_name]['labels'])
                print(f"{split_name} shape: {processed_data[split_name]['images'].shape}")
            else:
                print(f"Warning: No data found for {split_name}")

        return processed_data

    def save_preprocessed_data(self, processed_data: Dict, save_path: str) -> Dict:
        """
        Save preprocessed data to disk.

        Args:
            processed_data: Preprocessed data dictionary
            save_path: Path to save the data

        Returns:
            Metadata dictionary
        """
        print(f"Saving preprocessed data to: {save_path}")

        # Create save directory
        os.makedirs(save_path, exist_ok=True)

        # Save data for each split
        for split_name, data in processed_data.items():
            if data['images'].size > 0:
                split_folder = os.path.join(save_path, split_name)
                os.makedirs(split_folder, exist_ok=True)

                # Save images and labels
                np.save(os.path.join(split_folder, 'images.npy'), data['images'])
                np.save(os.path.join(split_folder, 'labels.npy'), data['labels'])

                print(f"  - {split_name}: {data['images'].shape[0]} samples saved")

        # Create metadata
        metadata = {
            'categories': self.categories,
            'label_encoder_classes': self.label_encoder.classes_.tolist(),
            'image_shape': list(processed_data['train']['images'].shape[1:]) if processed_data['train']['images'].size > 0 else list(self.target_size) + [3],
            'num_classes': len(self.categories),
            'price_calorie_mapping': self.price_calorie_map,
            'target_size': self.target_size,
            'augmentation_applied': {
                split_name: data['augmented'] for split_name, data in processed_data.items()
            },
            'dataset_info': {
                'total_samples': sum(data['images'].shape[0] for data in processed_data.values() if data['images'].size > 0),
                'splits': list(self.splits.keys())
            }
        }

        # Save metadata as JSON
        with open(os.path.join(save_path, 'metadata.json'), 'w') as f:
            json.dump(metadata, f, indent=2)

        # Save label encoder
        with open(os.path.join(save_path, 'label_encoder.pkl'), 'wb') as f:
            pickle.dump(self.label_encoder, f)

        print("Metadata and label encoder saved")
        return metadata

    def run_full_pipeline(self, save_path: str, apply_augmentation: bool = True) -> Tuple[Dict, Dict]:
        """
        Run the complete preprocessing pipeline.

        Args:
            save_path: Path to save preprocessed data
            apply_augmentation: Whether to apply augmentation to training data

        Returns:
            Tuple of (processed_data, metadata)
        """
        print("Starting full preprocessing pipeline...")

        # Step 1: Download dataset
        dataset_path = self.download_dataset()
        if not dataset_path:
            raise RuntimeError("Failed to download dataset")

        # Step 2: Load dataset structure
        if not self.load_dataset_structure(dataset_path):
            raise RuntimeError("Failed to load dataset structure")

        # Step 3: Preprocess dataset
        processed_data = self.preprocess_dataset(apply_augmentation)

        # Step 4: Save preprocessed data
        metadata = self.save_preprocessed_data(processed_data, save_path)

        print("Full preprocessing pipeline completed successfully!")
        return processed_data, metadata


def load_preprocessed_data(data_path: str) -> Tuple[Dict, Dict, LabelEncoder]:
    """
    Load preprocessed data from disk.

    Args:
        data_path: Path to preprocessed data directory

    Returns:
        Tuple of (data_dict, metadata, label_encoder)
    """
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Data path not found: {data_path}")

    # Load metadata
    metadata_path = os.path.join(data_path, 'metadata.json')
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)

    # Load label encoder
    encoder_path = os.path.join(data_path, 'label_encoder.pkl')
    with open(encoder_path, 'rb') as f:
        label_encoder = pickle.load(f)

    # Load data for each split
    data_dict = {}
    for split_name in ['train', 'test', 'validation']:
        split_path = os.path.join(data_path, split_name)
        if os.path.exists(split_path):
            images_path = os.path.join(split_path, 'images.npy')
            labels_path = os.path.join(split_path, 'labels.npy')

            if os.path.exists(images_path) and os.path.exists(labels_path):
                data_dict[split_name] = {
                    'images': np.load(images_path),
                    'labels': np.load(labels_path)
                }

    return data_dict, metadata, label_encoder


def preprocess_single_image_for_prediction(image_path: str, target_size: Tuple[int, int] = (224, 224)) -> Optional[np.ndarray]:
    """
    Preprocess a single image for prediction.

    Args:
        image_path: Path to the image file
        target_size: Target image size as (height, width)

    Returns:
        Preprocessed image array ready for prediction or None if failed
    """
    try:
        # Load image
        image = cv2.imread(image_path)
        if image is None:
            return None

        # Convert BGR to RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Resize image
        image = cv2.resize(image, target_size)

        # Normalize pixel values to [0,1]
        image = image.astype(np.float32) / 255.0

        # Add batch dimension for prediction
        image = np.expand_dims(image, axis=0)

        return image
    except Exception as e:
        print(f"Error processing image {image_path}: {e}")
        return None


def main():
    """
    Main function to demonstrate usage.
    """
    # Initialize preprocessor
    preprocessor = FruitVegetablePreprocessor()

    # Define save path
    save_path = "./preprocessed_data"

    try:
        # Run full pipeline
        processed_data, metadata = preprocessor.run_full_pipeline(save_path)

        # Print summary
        print("\nPreprocessing Summary:")
        print(f"  - Total categories: {metadata['num_classes']}")
        print(f"  - Image shape: {metadata['image_shape']}")
        print(f"  - Total samples: {metadata['dataset_info']['total_samples']}")
        print(f"  - Data saved to: {save_path}")

        return True

    except Exception as e:
        print(f"Error in preprocessing pipeline: {e}")
        return False


if __name__ == "__main__":
    main()