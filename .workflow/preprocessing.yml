name: Fruit & Vegetable Dataset Preprocessing

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'preprocessing/**'
      - '.workflow/**'
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      apply_augmentation:
        description: 'Apply data augmentation'
        required: false
        default: 'true'
        type: boolean
      target_size:
        description: 'Target image size (format: 224,224)'
        required: false
        default: '224,224'
        type: string
      upload_to_drive:
        description: 'Upload results to Google Drive'
        required: false
        default: 'false'
        type: boolean

jobs:
  preprocessing:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ['3.9']
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libgl1-mesa-glx \
          libglib2.0-0 \
          libsm6 \
          libxext6 \
          libxrender-dev \
          libgomp1 \
          libglu1-mesa-dev \
          freeglut3-dev \
          mesa-common-dev
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Set up Kaggle credentials
      env:
        KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
        KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
      run: |
        mkdir -p ~/.kaggle
        echo '{"username":"${{ secrets.KAGGLE_USERNAME }}","key":"${{ secrets.KAGGLE_KEY }}"}' > ~/.kaggle/kaggle.json
        chmod 600 ~/.kaggle/kaggle.json
        
    - name: Set up Google Drive credentials (optional)
      if: ${{ github.event.inputs.upload_to_drive == 'true' }}
      env:
        GDRIVE_CREDENTIALS: ${{ secrets.GDRIVE_CREDENTIALS }}
      run: |
        mkdir -p ~/.config/gdrive
        echo '${{ secrets.GDRIVE_CREDENTIALS }}' > ~/.config/gdrive/credentials.json
        
    - name: Create required directories
      run: |
        mkdir -p namadataset_raw
        mkdir -p preprocessing/namadataset_preprocessing
        ls -la
        
    - name: Parse workflow inputs
      id: parse_inputs
      run: |
        if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
          echo "augmentation=${{ github.event.inputs.apply_augmentation }}" >> $GITHUB_OUTPUT
          echo "target_size=${{ github.event.inputs.target_size }}" >> $GITHUB_OUTPUT
          echo "upload_drive=${{ github.event.inputs.upload_to_drive }}" >> $GITHUB_OUTPUT
        else
          echo "augmentation=true" >> $GITHUB_OUTPUT
          echo "target_size=224,224" >> $GITHUB_OUTPUT
          echo "upload_drive=false" >> $GITHUB_OUTPUT
        fi
        
    - name: Update automate_preprocessing.py for custom paths
      run: |
        cd preprocessing
        python -c "
import re

# Read the original file
with open('automate_preprocessing.py', 'r') as f:
    content = f.read()

# Update paths in the script
# Change dataset download path
content = re.sub(
    r'save_path = \"\.\/preprocessed_data\"',
    'save_path = \"./namadataset_preprocessing\"',
    content
)

# Update main function to use correct paths
main_function_update = '''
def main():
    \"\"\"
    Main function to demonstrate usage.
    \"\"\"
    import os
    
    # Initialize preprocessor
    preprocessor = FruitVegetablePreprocessor()

    # Define save path relative to preprocessing folder
    save_path = \"./namadataset_preprocessing\"

    try:
        # Run full pipeline
        processed_data, metadata = preprocessor.run_full_pipeline(save_path)

        # Print summary
        print(\"\\nPreprocessing Summary:\")
        print(f\"  - Total categories: {metadata['num_classes']}\")
        print(f\"  - Image shape: {metadata['image_shape']}\")
        print(f\"  - Total samples: {metadata['dataset_info']['total_samples']}\")
        print(f\"  - Data saved to: {save_path}\")

        return True

    except Exception as e:
        print(f\"Error in preprocessing pipeline: {e}\")
        return False
'''

# Replace the main function
content = re.sub(
    r'def main\(\):.*?return False',
    main_function_update.strip(),
    content,
    flags=re.DOTALL
)

# Write updated content
with open('automate_preprocessing.py', 'w') as f:
    f.write(content)

print('Updated automate_preprocessing.py with correct paths')
"
        
    - name: Run preprocessing
      env:
        APPLY_AUGMENTATION: ${{ steps.parse_inputs.outputs.augmentation }}
        TARGET_SIZE: ${{ steps.parse_inputs.outputs.target_size }}
      run: |
        cd preprocessing
        python -c "
import sys
import os
import json
from datetime import datetime
sys.path.append('.')
from automate_preprocessing import FruitVegetablePreprocessor

# Parse target size
target_size_str = os.environ.get('TARGET_SIZE', '224,224')
target_size = tuple(map(int, target_size_str.split(',')))

# Parse augmentation flag
apply_augmentation = os.environ.get('APPLY_AUGMENTATION', 'true').lower() == 'true'

print(f'Starting preprocessing with target_size={target_size}, augmentation={apply_augmentation}')

# Initialize preprocessor
preprocessor = FruitVegetablePreprocessor(target_size=target_size)

# Run preprocessing
try:
    processed_data, metadata = preprocessor.run_full_pipeline(
        save_path='./namadataset_preprocessing',
        apply_augmentation=apply_augmentation
    )
    
    # Save run info
    run_info = {
        'status': 'success',
        'timestamp': datetime.now().isoformat(),
        'commit_sha': os.environ.get('GITHUB_SHA', 'unknown'),
        'branch': os.environ.get('GITHUB_REF_NAME', 'unknown'),
        'target_size': target_size,
        'augmentation_applied': apply_augmentation,
        'total_samples': metadata['dataset_info']['total_samples'],
        'num_classes': metadata['num_classes'],
        'categories': metadata['categories'],
        'preprocessing_path': './namadataset_preprocessing'
    }
    
    # Save to preprocessing folder
    with open('./namadataset_preprocessing/run_info.json', 'w') as f:
        json.dump(run_info, f, indent=2)
        
    print('Preprocessing completed successfully!')
    print(f'Results saved to: ./namadataset_preprocessing')
    
except Exception as e:
    print(f'Error during preprocessing: {e}')
    run_info = {
        'status': 'failed',
        'timestamp': datetime.now().isoformat(),
        'error': str(e)
    }
    os.makedirs('./namadataset_preprocessing', exist_ok=True)
    with open('./namadataset_preprocessing/run_info.json', 'w') as f:
        json.dump(run_info, f, indent=2)
    sys.exit(1)
"
        
    - name: Generate preprocessing summary
      run: |
        cd preprocessing
        python -c "
import json
import os
from datetime import datetime

# Load run info
try:
    with open('./namadataset_preprocessing/run_info.json', 'r') as f:
        run_info = json.load(f)
except:
    print('No run info found')
    exit(1)

# Generate summary
summary = f'''# üçéü•ï Preprocessing Summary Report

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
**Status:** {'‚úÖ SUCCESS' if run_info['status'] == 'success' else '‚ùå FAILED'}
**Commit:** {run_info.get('commit_sha', 'Unknown')[:8]}
**Branch:** {run_info.get('branch', 'Unknown')}

## üìä Dataset Information
- **Total Samples:** {run_info.get('total_samples', 'N/A'):,}
- **Number of Classes:** {run_info.get('num_classes', 'N/A')}
- **Target Image Size:** {run_info.get('target_size', 'N/A')}
- **Data Augmentation:** {'Enabled' if run_info.get('augmentation_applied') else 'Disabled'}

## üè∑Ô∏è Categories
{', '.join(run_info.get('categories', []))}

## üìÅ File Structure
preprocessing/namadataset_preprocessing/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ images.npy
‚îÇ   ‚îî‚îÄ‚îÄ labels.npy
‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ images.npy
‚îÇ   ‚îî‚îÄ‚îÄ labels.npy
‚îú‚îÄ‚îÄ validation/
‚îÇ   ‚îú‚îÄ‚îÄ images.npy
‚îÇ   ‚îî‚îÄ‚îÄ labels.npy
‚îú‚îÄ‚îÄ metadata.json
‚îú‚îÄ‚îÄ label_encoder.pkl
‚îî‚îÄ‚îÄ run_info.json

## üíæ Storage Information
'''

# Add file sizes
try:
    for root, dirs, files in os.walk('./namadataset_preprocessing'):
        for file in files:
            if file.endswith(('.npy', '.json', '.pkl')):
                filepath = os.path.join(root, file)
                size = os.path.getsize(filepath)
                relative_path = os.path.relpath(filepath, './namadataset_preprocessing')
                if size > 1024*1024:  # > 1MB
                    summary += f'- **{relative_path}:** {size/(1024*1024):.1f} MB\\n'
                else:
                    summary += f'- **{relative_path}:** {size/1024:.1f} KB\\n'
except Exception as e:
    summary += f'Error getting file sizes: {e}\\n'

# Save summary
with open('./namadataset_preprocessing/PREPROCESSING_SUMMARY.md', 'w') as f:
    f.write(summary)

print('Generated preprocessing summary')
print(summary)
"
        
    - name: List processed files structure
      run: |
        echo "=== üìÅ Directory Structure ==="
        tree preprocessing/namadataset_preprocessing/ || find preprocessing/namadataset_preprocessing/ -type f
        echo ""
        echo "=== üìä File Sizes ==="
        du -sh preprocessing/namadataset_preprocessing/* 2>/dev/null || echo "No files found"
        echo ""
        echo "=== üîç Sample Files Content ==="
        ls -la preprocessing/namadataset_preprocessing/
        
    - name: Validate preprocessing results
      run: |
        cd preprocessing
        python -c "
import os
import json
import numpy as np

print('üîç Validating preprocessing results...')

# Check if all required files exist
required_files = [
    'namadataset_preprocessing/metadata.json',
    'namadataset_preprocessing/label_encoder.pkl',
    'namadataset_preprocessing/run_info.json'
]

missing_files = []
for file in required_files:
    if not os.path.exists(file):
        missing_files.append(file)

if missing_files:
    print(f'‚ùå Missing files: {missing_files}')
    exit(1)

# Check data splits
splits_found = []
for split in ['train', 'test', 'validation']:
    split_dir = f'namadataset_preprocessing/{split}'
    if os.path.exists(split_dir):
        images_file = f'{split_dir}/images.npy'
        labels_file = f'{split_dir}/labels.npy'
        
        if os.path.exists(images_file) and os.path.exists(labels_file):
            # Load and check shapes
            images = np.load(images_file)
            labels = np.load(labels_file)
            
            if images.shape[0] == labels.shape[0]:
                splits_found.append(f'{split}: {images.shape[0]} samples, shape: {images.shape}')
            else:
                print(f'‚ùå Shape mismatch in {split}: images={images.shape[0]}, labels={labels.shape[0]}')
                exit(1)

print('‚úÖ Validation passed!')
print('üìä Data splits found:')
for split in splits_found:
    print(f'  - {split}')
"
        
    - name: Create workflow artifacts
      run: |
        # Create a summary artifact folder
        mkdir -p workflow_artifacts
        
        # Copy important files
        cp preprocessing/namadataset_preprocessing/PREPROCESSING_SUMMARY.md workflow_artifacts/
        cp preprocessing/namadataset_preprocessing/run_info.json workflow_artifacts/
        cp preprocessing/namadataset_preprocessing/metadata.json workflow_artifacts/
        
        # Create a simple dataset info file
        cd preprocessing
        python -c "
import json
import os

# Get directory size
def get_dir_size(path):
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            total += os.path.getsize(fp)
    return total

dataset_info = {
    'dataset_path': 'preprocessing/namadataset_preprocessing/',
    'total_size_mb': round(get_dir_size('./namadataset_preprocessing') / (1024*1024), 2),
    'files_created': []
}

# List all created files
for root, dirs, files in os.walk('./namadataset_preprocessing'):
    for file in files:
        filepath = os.path.join(root, file)
        rel_path = os.path.relpath(filepath, './namadataset_preprocessing')
        dataset_info['files_created'].append({
            'file': rel_path,
            'size_mb': round(os.path.getsize(filepath) / (1024*1024), 3)
        })

with open('../workflow_artifacts/dataset_info.json', 'w') as f:
    json.dump(dataset_info, f, indent=2)
"
        
    - name: Upload preprocessing results as artifacts
      uses: actions/upload-artifact@v3
      with:
        name: fruit-veg-preprocessed-dataset-${{ github.run_number }}
        path: |
          preprocessing/namadataset_preprocessing/
        retention-days: 30
        
    - name: Upload workflow summary
      uses: actions/upload-artifact@v3
      with:
        name: preprocessing-summary-${{ github.run_number }}
        path: workflow_artifacts/
        retention-days: 7
        
    - name: Optional - Upload to Google Drive
      if: ${{ steps.parse_inputs.outputs.upload_drive == 'true' && secrets.GDRIVE_CREDENTIALS }}
      run: |
        echo "üöÄ Uploading to Google Drive..."
        # Install gdrive tool
        pip install google-drive-api-python
        
        cd preprocessing
        python -c "
import os
import zipfile
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload

print('Creating zip archive...')
# Create zip file
with zipfile.ZipFile('namadataset_preprocessing.zip', 'w', zipfile.ZIP_DEFLATED) as zipf:
    for root, dirs, files in os.walk('namadataset_preprocessing'):
        for file in files:
            file_path = os.path.join(root, file)
            arcname = os.path.relpath(file_path, '.')
            zipf.write(file_path, arcname)

print('Zip created successfully!')
print(f'Size: {os.path.getsize(\"namadataset_preprocessing.zip\") / (1024*1024):.2f} MB')
"
        
    - name: Create release on main branch
      if: success() && github.ref == 'refs/heads/main'
      uses: ncipollo/release-action@v1
      with:
        tag: dataset-v${{ github.run_number }}
        name: "üçéü•ï Preprocessed Dataset v${{ github.run_number }}"
        bodyFile: "workflow_artifacts/PREPROCESSING_SUMMARY.md"
        artifacts: "preprocessing/namadataset_preprocessing/*.json,preprocessing/namadataset_preprocessing/*.pkl"
        token: ${{ secrets.GITHUB_TOKEN }}
        draft: false
        prerelease: false
        
    - name: Post-processing cleanup and summary
      if: always()
      run: |
        echo "=== üéØ Final Summary ==="
        echo "Workflow Status: ${{ job.status }}"
        echo "Repository: ${{ github.repository }}"
        echo "Branch: ${{ github.ref_name }}"
        echo "Commit: ${{ github.sha }}"
        echo "Run Number: ${{ github.run_number }}"
        echo ""
        echo "=== üìÇ Generated Artifacts ==="
        echo "1. fruit-veg-preprocessed-dataset-${{ github.run_number }}"
        echo "2. preprocessing-summary-${{ github.run_number }}"
        echo ""
        if [ "${{ github.ref }}" == "refs/heads/main" ]; then
          echo "üéâ Release created: dataset-v${{ github.run_number }}"
        fi
